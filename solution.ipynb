{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclic Coordinate Descent for Logistic Regression with Lasso regularization\n",
    "\n",
    "This notebook presents the implementation of Cyclic Coordinate Descent (CCD) algorithm for parameter \n",
    "estimation in regularized logistic regression with l1 (lasso) penalty and compares it with standard \n",
    "logistic regression model without regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"cubehelix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_DATASET_DIRECTORY_PATH = \"./datasets\"\n",
    "CONST_RESULTS_DIRECTORY_PATH = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility (More details in the README.md)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Find 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets() -> List[dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Load all ARFF datasets from the datasets folder and return them as a list of polars dataframes.\"\"\"\n",
    "    datasets = []\n",
    "    for file in os.listdir(CONST_DATASET_DIRECTORY_PATH):\n",
    "        if file.endswith(\".arff\"):\n",
    "            data = arff.loadarff(f\"{CONST_DATASET_DIRECTORY_PATH}/{file}\")\n",
    "            df = pd.DataFrame(data[0])\n",
    "            datasets.append({\"name\": file.strip(\".arff\"), \"data\": df})\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a dataset from a given path and return it as a polars dataframe.\"\"\"\n",
    "    data = arff.loadarff(path)\n",
    "    return pd.DataFrame(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Represents the dataset with the name, features, target, and preprocessing steps.\n",
    "    Features and target are available as numpy arrays after preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, name: str, df: pd.DataFrame, preprocessing_steps: List[callable] = None\n",
    "    ):\n",
    "        \"\"\"Initialize a new dataset with a name, data, and preprocess the data resulting in two numpy arrays. X - features, and y - target.\"\"\"\n",
    "\n",
    "        self.name = name\n",
    "        self.preprocessing_steps = preprocessing_steps\n",
    "\n",
    "        self.X = df[df.columns[:-1]]\n",
    "        self.y = df[df.columns[-1]]\n",
    "\n",
    "        for step in self.preprocessing_steps:\n",
    "            self.X = step(self.X)\n",
    "\n",
    "        # Improve Logistic Regression performance by converting to numpy arrays\n",
    "        self.X = self.X.to_numpy()\n",
    "\n",
    "        # Convert the target to binary values\n",
    "        self.class_names = self.y.unique()\n",
    "\n",
    "        # To mitigate CopyOnWriteWarning\n",
    "        self.y = self.y.copy()\n",
    "        self.y[self.y == self.class_names[0]] = 0\n",
    "        self.y[self.y == self.class_names[1]] = 1\n",
    "        self.y = self.y.to_numpy()\n",
    "\n",
    "    def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fill the missing values in the dataframe using the mean of the column strategy.\"\"\"\n",
    "        return df.fillna(df.mean())\n",
    "\n",
    "    def remove_colinear_features(\n",
    "        df: pd.DataFrame, threshold: float = 0.8\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Remove features of a dataframe that are colinear.\"\"\"\n",
    "\n",
    "        corr_matrix = df.corr().abs()\n",
    "\n",
    "        upper_tri = corr_matrix.where(\n",
    "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "\n",
    "        to_drop = [\n",
    "            column for column in upper_tri.columns if any(upper_tri[column] > threshold)\n",
    "        ]\n",
    "\n",
    "        return df.drop(columns=to_drop)\n",
    "\n",
    "    def normalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Normalize the features of a dataframe based on Min-Max.\"\"\"\n",
    "\n",
    "        # Use the Min-Max normalization to produce features in range [0, 1]\n",
    "\n",
    "        return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "\n",
    "class APBreastKidney(Dataset):\n",
    "    \"\"\"APBreastKidney dataset.\n",
    "    source: https://www.openml.org/search?type=data&sort=runs&id=1158&status=active\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_path = f\"{CONST_DATASET_DIRECTORY_PATH}/AP_Breast_Kidney.arff\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize tha APBreastKidney dataset\"\"\"\n",
    "\n",
    "        data = load_dataset(APBreastKidney.dataset_path)\n",
    "\n",
    "        super().__init__(\n",
    "            \"APBreastKidney\",\n",
    "            data,\n",
    "            [\n",
    "                Dataset.fill_missing_values,\n",
    "                Dataset.remove_colinear_features,\n",
    "                Dataset.normalize,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "\n",
    "    def __init__(self, p=0.5, n=1000, d=10, g=0.1):\n",
    "        \"\"\"Initialize the synthetic dataset with the given parameters.\"\"\"\n",
    "\n",
    "        # Generate binary class variable (Y=0 or Y=1) from Bernoulli distribution with class prior probability p.\n",
    "\n",
    "        y = np.random.binomial(1, p, n)\n",
    "\n",
    "        # Generate feature vector X, such that X|Y=0 follows d-dimensional multivariate normal distribution with mean (0,â€¦,0) and covariance matrix S with ð‘†[â…ˆ,ð‘—] = ð‘”^|â…ˆâˆ’ð‘—| , whereas X|Y=1 follows d-dimensional multivariate normal distribution with mean (1,1/2,1/3,â€¦1/d) and covariance matrix ð‘†[â…ˆ, ð‘—] = ð‘”^|â…ˆâˆ’ð‘—|.\n",
    "\n",
    "        X = np.zeros((n, d))\n",
    "\n",
    "        X[y == 0] = np.random.multivariate_normal(\n",
    "            np.zeros(d),  # mean\n",
    "            np.array(\n",
    "                [[np.pow(g, np.abs(i - j)) for i in range(d)] for j in range(d)]\n",
    "            ),  # covariance matrix\n",
    "            sum(y == 0),  # number of samples for class 0\n",
    "        )\n",
    "        X[y == 1] = np.random.multivariate_normal(\n",
    "            np.array([1 / (i + 1) for i in range(d)]),  # mean\n",
    "            np.array(\n",
    "                [[np.pow(g, np.abs(i - j)) for i in range(d)] for j in range(d)]\n",
    "            ),  # covariance matrix\n",
    "            sum(y == 1),  # number of samples for class 1\n",
    "        )\n",
    "\n",
    "        # Concatenate the features and target to a single dataframe\n",
    "        data = np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "        super().__init__(\n",
    "            \"SyntheticDataset\",\n",
    "            data,\n",
    "            [\n",
    "                Dataset.normalize,\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 7.63 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Keep in mind removing colinear features on a dataset with couple thousands of them is relatively time consuming\n",
    "\n",
    "# datasets = [APBreastKidney()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 4.77 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Alternatively, load all ARFF datasets from the datasets folder\n",
    "# As a result you will obtain a list Datasets objects\n",
    "\n",
    "# datasets = load_datasets()\n",
    "\n",
    "# preprocessing_steps = [\n",
    "#     Dataset.fill_missing_values,\n",
    "#     Dataset.remove_colinear_features,\n",
    "#     Dataset.normalize,\n",
    "# ]\n",
    "\n",
    "# for i in range(len(datasets)):\n",
    "#     datasets[i] = Dataset(datasets[i][\"name\"], datasets[i][\"data\"], preprocessing_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogRegCCD\n",
    "\n",
    "Implementation of regularized Logistic Regression wiht Cyclic Coordinate Descent based on the [publication](https://www.jstatsoft.org/article/view/v033i01) (Chapter 3 is most relevant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Cleanup this description\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a machine learning method capable of binary classification. It predicts the probability of an outcome. It's steps are as follows:\n",
    "\n",
    "Compute a linear combination of input features:\n",
    "\n",
    "$$\n",
    "z = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$ denotes input feature vector, where $x_1,...,x_n$ are the elements of that vector\n",
    "- $w$ denotes model weights vector \n",
    "- $b$ is the bias term  \n",
    "\n",
    "The output $z$ is then provided to the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The output in range [0,1] denotes the probability that given feature vector $x$ belongs to the positive class.\n",
    "\n",
    "Prediction rule is based on the output of the sigmoid function, if it's larger than 0.5 we assign to class 1, otherwise assign to class 0.\n",
    "\n",
    "To fit the model to the training data. One needs to minimize the loss function, in this case Binary Cross-Entropy:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\hat{y}^{(i)} + \\left(1 - y^{(i)}\\right) \\log \\left(1 - \\hat{y}^{(i)}\\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ denotes the number of training examples  \n",
    "- $y^{(i)}$ is the class label \n",
    "- $\\hat{y}^{(i)}$ is the predicted probability  \n",
    "\n",
    "\n",
    "The weights of the model need to be optimized to find the proper fit, this can be achieved by standard gradient descent:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha $ is the learning rate, the higher the value the more aggressive weight updates  \n",
    "- $ \\frac{\\partial \\mathcal{L}}{\\partial w_j} $ is a gradient with respect to weight $ w_j $\n",
    "\n",
    "\n",
    "**Lasso Regulaization** (L1) is used to prevent overfitting, when the trained model can predict samples from the training set very well but struggles on the test set.\n",
    "\n",
    "Then the loss function becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{lasso}} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\hat{y}^{(i)} + \\left(1 - y^{(i)}\\right) \\log \\left(1 - \\hat{y}^{(i)}\\right) \\right] + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m $  is the number of training samples  \n",
    "- $ y^{(i)} $ is the class label\n",
    "- $ \\hat{y}^{(i)} $ is the predicted probability \n",
    "- $ \\lambda $ denotes regularization strength \n",
    "\n",
    "In essence during the training process, the model will also minizem the absolute sum of the coefficients in addition to the loss function\n",
    "\n",
    "Now, to use the Cyclic Coordinate Descent instead of the standard Gradient Descent one needs to minimize the $\\mathcal{L}_{\\text{lasso}}$ using a different algorithm for updating model weights.\n",
    "\n",
    "However the authors of the [publication](https://www.jstatsoft.org/article/view/v033i01) present a more sophisticated approach with certain optimizations so let us focus on them first.\n",
    "\n",
    "First, the Logistic Regression log-likelihood:\n",
    "\n",
    "$$\n",
    "\\max_{\\beta} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right] - \\lambda ||\\beta||_1\n",
    "$$\n",
    "\n",
    "is approximated with a quadratic approximation:\n",
    "\n",
    "$$\n",
    "\\ell_Q(\\beta_0, \\beta) = -\\frac{1}{2N} \\sum_{i=1}^{N} w_i (z_i - \\beta_0 - x_i^T \\beta)^2 + C\n",
    "$$\n",
    "\n",
    "That converts the problem into a **penalized weighted least squares (WLS)**.\n",
    "\n",
    "The authors also use a **regularization path** that starts from **largest $\\lambda$** where $\\beta = 0$ and decreases $\\lambda$ gradually, using previous solutions as warm starts.\n",
    "\n",
    "Instead of computing gradients from scratch with each iteration, the authors propose to use **covariance updates**:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} x_{ij} r_i = \\langle x_j, y \\rangle - \\sum_{k: \\beta_k \\neq 0} \\langle x_j, x_k \\rangle \\beta_k\n",
    "$$\n",
    "\n",
    "For each feature $ j $, the optimization problem simplifies to:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_j} \\left[ \\frac{1}{2} \\sum_{i=1}^{N} w_i \\left( z_i - \\beta_0 - \\sum_{k \\neq j} x_{ik} \\beta_k - x_{ij} \\beta_j \\right)^2 + \\lambda |\\beta_j| \\right]\n",
    "$$\n",
    "\n",
    "To update the weights using CCD one needs to (for weight j):\n",
    "1. **Compute partial residuals** (excluding $ \\beta_j $):\n",
    "\n",
    "   $$\n",
    "   r_i = z_i - (\\beta_0 + \\sum_{k \\neq j} x_{ik} \\beta_k)\n",
    "   $$\n",
    "\n",
    "2. **Compute the gradient component $ \\rho_j $:**\n",
    "\n",
    "   $$\n",
    "   \\rho_j = \\sum_{i=1}^{N} w_i x_{ij} r_i\n",
    "   $$\n",
    "\n",
    "3. **Apply soft-thresholding for L1 regularization:**\n",
    "\n",
    "   $$\n",
    "   \\beta_j = \\frac{S(\\rho_j, \\lambda)}{\\sum_{i=1}^{N} w_i x_{ij}^2}\n",
    "   $$\n",
    "\n",
    "   where the **soft-thresholding operator** is:\n",
    "\n",
    "   $$\n",
    "   S(z, \\lambda) = \\text{sign}(z) \\cdot \\max(|z| - \\lambda, 0)\n",
    "   $$\n",
    "\n",
    "And then $\\beta_0$ that is not regularized:\n",
    "$$\n",
    "\\beta_0 = \\frac{\\sum_{i=1}^{N} w_i (z_i - x_i^T \\beta)}{\\sum_{i=1}^{N} w_i}\n",
    "$$\n",
    "\n",
    "\n",
    "From a high level overview the presented algorithm consists of:\n",
    "\n",
    "1. Outer Loop:\n",
    "   - Decrease $ \\lambda $ along a regularization path.\n",
    "\n",
    "2. Middle Loop:\n",
    "   - Update the quadratic approximation using the current $ (\\beta_0, \\beta) $.\n",
    "\n",
    "3. Inner Loop:\n",
    "   - Perform **coordinate descent** on the **penalized weighted least squares problem**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegCCD:\n",
    "    \"\"\"Logistic Regression with Coordinate Cyclic Descent and Lasso Regularization.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the LogRegCCD model.\"\"\"\n",
    "\n",
    "        self.beta = None\n",
    "        self.beta_0 = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        regularization_path_lambda_sequence: np.ndarray = np.logspace(-2, 2, 10),\n",
    "        maximum_iterations: int = 1000,\n",
    "        convergance_threshold: float = 1e-05,\n",
    "        minimum: float = 1e-05,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the Logsitic Regression model on provided training features and labels.\"\"\"\n",
    "\n",
    "        _, number_of_features = X_train.shape\n",
    "        self.beta = np.zeros(number_of_features)  # Initialize coefficients\n",
    "        self.beta_0 = 0  # Initialize intercept\n",
    "\n",
    "        for (\n",
    "            lambda_\n",
    "        ) in (\n",
    "            regularization_path_lambda_sequence\n",
    "        ):  # Outer Loop: Decrease $ \\lambda $ along a regularization path.\n",
    "\n",
    "            for _ in range(\n",
    "                maximum_iterations\n",
    "            ):  # Middle Loop: Update the quadratic approximation using the current $ (\\beta_0, \\beta) $.\n",
    "\n",
    "                previous_beta = self.beta.copy()\n",
    "                previous_beta_0 = self.beta_0\n",
    "\n",
    "                # Linear Predictions and Probabilities\n",
    "\n",
    "                linear_prediction = self.beta_0 + np.dot(X_train, self.beta)\n",
    "                probabilities = self.sigmoid(linear_prediction)\n",
    "\n",
    "                # Weights and Working Response\n",
    "\n",
    "                weights = np.maximum(minimum, probabilities * (1 - probabilities))\n",
    "                working_response = linear_prediction + np.divide(\n",
    "                    y_train - probabilities, weights, where=weights != 0\n",
    "                )\n",
    "\n",
    "                # Update the intercept \\beta_0\n",
    "\n",
    "                self.beta_0 = np.divide(\n",
    "                    np.sum(weights * (working_response - np.dot(X_train, self.beta))),\n",
    "                    np.sum(weights),\n",
    "                    where=np.sum(weights) != 0,\n",
    "                )\n",
    "\n",
    "                # Coordinate Descent for each feature\n",
    "\n",
    "                for feature_index in range(\n",
    "                    number_of_features\n",
    "                ):  # Inner Loop: Update the coefficients $ \\beta $\n",
    "\n",
    "                    # Compute the partial residual\n",
    "\n",
    "                    partial_residual = (\n",
    "                        working_response\n",
    "                        - self.beta_0\n",
    "                        - np.dot(X_train, self.beta)\n",
    "                        + X_train[:, feature_index] * self.beta[feature_index]\n",
    "                    )\n",
    "\n",
    "                    # Compute the coordinate-wise derivative\n",
    "\n",
    "                    coordinate_derivative = np.dot(\n",
    "                        weights * X_train[:, feature_index], partial_residual\n",
    "                    )\n",
    "\n",
    "                    # Soft-thresholding\n",
    "\n",
    "                    denominator = np.sum(weights * X_train[:, feature_index] ** 2)\n",
    "                    self.beta[feature_index] = np.divide(\n",
    "                        self.soft_thresholding(coordinate_derivative, lambda_),\n",
    "                        denominator,\n",
    "                        where=denominator != 0,\n",
    "                    )\n",
    "\n",
    "                # Check for convergence\n",
    "\n",
    "                if (\n",
    "                    np.linalg.norm(self.beta - previous_beta) < convergance_threshold\n",
    "                    and np.abs(self.beta_0 - previous_beta_0) < convergance_threshold\n",
    "                ):\n",
    "                    break\n",
    "\n",
    "    def validate(\n",
    "        self, X_valid: np.ndarray, y_valid: np.ndarray, measure: callable\n",
    "    ) -> float:\n",
    "        \"\"\"Compute the provided measure based on the validation features and labels.\"\"\"\n",
    "        return measure(y_valid, self.predict(X_valid))\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the classes for the test features.\"\"\"\n",
    "        return self.predict_proba(X_test) > 0.5\n",
    "\n",
    "    def predict_proba(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the probabilities of the classes for the test features.\"\"\"\n",
    "        return self.sigmoid(self.beta_0 + np.dot(X_test, self.beta))\n",
    "\n",
    "    def plot(self, measure: str) -> None:\n",
    "        \"\"\"Plot the evalueation measure over different values of lambda.\"\"\"\n",
    "        # TODO: What should we do here exactly?\n",
    "        pass\n",
    "\n",
    "    def plot_coefficients(self) -> None:\n",
    "        \"\"\"Plot the coefficients of the model over different values of lambda.\"\"\"\n",
    "        # TODO: We should store the coefficients over different values of lambda\n",
    "        pass\n",
    "\n",
    "    # Helper functions\n",
    "\n",
    "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Returns the sigmoid function result.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def soft_thresholding(self, z: float, lambda_: float) -> float:\n",
    "        \"\"\"Applies soft-thresholding for L1\"\"\"\n",
    "        return np.sign(z) * np.maximum(np.abs(z) - lambda_, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_arr = [0.1, 0.5]\n",
    "n_arr = [\n",
    "    1000,\n",
    "]\n",
    "d_arr = [10]\n",
    "g_arr = [0.1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.19 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: Performance and Comparison according to the Project Task\n",
    "\n",
    "results = []\n",
    "\n",
    "for p in p_arr:\n",
    "    for n in n_arr:\n",
    "        for d in d_arr:\n",
    "            for g in g_arr:\n",
    "                synthetic_dataset = SyntheticDataset(p, n, d, g)\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    synthetic_dataset.X,\n",
    "                    synthetic_dataset.y,\n",
    "                    test_size=0.2,\n",
    "                    random_state=42,\n",
    "                )\n",
    "\n",
    "                log_reg_ccd = LogRegCCD()\n",
    "                log_reg_ccd.fit(X_train, y_train)\n",
    "\n",
    "                accuracy_log_reg_ccd = accuracy_score(\n",
    "                    y_test, log_reg_ccd.predict(X_test)\n",
    "                )\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"p\": p,\n",
    "                        \"n\": n,\n",
    "                        \"d\": d,\n",
    "                        \"g\": g,\n",
    "                        \"metric_value\": accuracy_log_reg_ccd,\n",
    "                        \"metric_name\": \"Accuracy\",\n",
    "                        \"model\": \"LogRegCCD\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                log_sklearn = LogisticRegression(penalty=None)\n",
    "                log_sklearn.fit(X_train, y_train)\n",
    "                accuracy_log_sklearn = accuracy_score(\n",
    "                    y_test, log_sklearn.predict(X_test)\n",
    "                )\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"p\": p,\n",
    "                        \"n\": n,\n",
    "                        \"d\": d,\n",
    "                        \"g\": g,\n",
    "                        \"metric_value\": accuracy_log_sklearn,\n",
    "                        \"metric_name\": \"Accuracy\",\n",
    "                        \"model\": \"LogReg sklearn\",\n",
    "                    }\n",
    "                )\n",
    "                del synthetic_dataset\n",
    "                del log_reg_ccd\n",
    "                del log_sklearn\n",
    "                del X_train\n",
    "                del X_test\n",
    "                del y_train\n",
    "                del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_RESULTS_FILE_NAME = \"results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\n",
    "    f\"{CONST_RESULTS_DIRECTORY_PATH}/{CONST_RESULTS_FILE_NAME}\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del results_df\n",
    "del results\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(f\"{CONST_RESULTS_DIRECTORY_PATH}/{CONST_RESULTS_FILE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=results_df, x=\"model\", y=\"metric_value\")\n",
    "plt.title(\"Comparison of LogRegCCD and LogReg sklearn\")\n",
    "plt.savefig(f\"{CONST_RESULTS_DIRECTORY_PATH}/comparison.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
