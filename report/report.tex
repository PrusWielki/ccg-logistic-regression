\documentclass[11pt]{article}

\usepackage[margin=0.8in]{geometry}
\usepackage{titlesec}
\usepackage[hidelinks]{hyperref}
\usepackage{adjustbox}
\usepackage{float}
\usepackage{graphicx} %To include PDF as an image
\usepackage{times}
\usepackage{svg} % To include images in SVG format
\usepackage{ragged2e} % To justify abstract
\usepackage[T1]{fontenc}
\usepackage{comment} % For comment blocks
\usepackage[
    sorting=none
]{biblatex}
\usepackage{amsmath}
\usepackage{longtable}
\addbibresource{references.bib}


\titleformat*{\section}{\large\bfseries}

\begin{document}
\hypersetup{pdfauthor={Prusak, Patryk;}, pdftitle={}}

\title{Prusak_Patryk_ADVML_Project_1}
\author{\normalsize Patryk Prusak }
\hspace{0pt}
\vfill
\begin{center}

    \textbf{\huge{Cyclic Coordinate Descent for Logistic Regression with Lasso regularization}}
    
\end{center}

\vspace{0.5cm}
\begin{center}
    \normalsize{Patryk Prusak}
\end{center}
\vspace{0.5cm}
\begin{center}
    \normalsize{supervisor}\\
    \vspace{0.3cm}
    \normalsize{mgr Katarzyna Woźnica}
\end{center}

\vspace{0.5cm}

\begin{center} 
\normalsize{Warsaw University of Technology} \\
\vspace{0.3cm}
\normalsize{\today} \\
\vspace{0.3cm}
Advanced Machine Learning Course
\end{center}

\vspace{1cm}


\tableofcontents
\vfill
\hspace{0pt}
\newpage

%TODO:
% Every point described below should be included in separate section. Maximal length of report is 6 pages A4 (title page and refernces are not included in the limit). Report should include:
% • Methodology.
% o Selection and generation of datasets.
% o Details about algorithm implementation and applied optimizations
% • Discussion about correctness of the LogRegCCD algorithm.
% Suggested approach to address this point:
% o Performance of the algorithm at lambda=0
% o Likelihood function values and coefficient values depending on iteration
% o Comparison with ready implementation of logistic regression with L1 penalty
% • Impact of dataset parameters: n.p,d,g on the performance of LogRegCCD algorithm.
% • Benchmark of LogRegCCD with LogisticRegression algorithm.
% Suggested approach to address this point:
% o Performance of algorithms regarding different metrics
% o Values of coefficients obtained in these two methods

\section{Methodology}

\subsection{Selection and generation of datasets}

The implemented Cyclic Coordinate Descent for Logsitic Regression with Lasso regularization (LogRegCCD) algorithm was tested on both real and synthetic datasets to evaluate its performance given various metrics such as accuracy, precision, recall, F1 score, balanced accuracy and ROC AUC \cite{powers2011evaluation, brodersen2010balanced, fawcett2006introduction} provided by the scikit-learn \cite{scikit-learn} Python \cite{python} library. \par 



The synthetic datasets have been created with various values of the following parameters: class prior probability $p$, number of samples $n$, number of features $d$, and the covariance matrix $g$ parameter defined as $S[i,j] = g^{|i-j|}$. We have examined all unique combinations of the following parameter values: $p \in \{0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}$, $n \in \{1000, 1500, 2000\}$, $d \in \{2, 5, 10, 30\}$ and $g \in \{0.1, 0.3, 0.5, 0.7, 0.9\}$. For $p$ and $g$ we have examined more parameter values to further investigate certain trends discussed in further sections. The synthetic datasets were generated according to the task description, that is:

To generate the dataset, we first draw a binary class variable $Y$ from a Bernoulli distribution with class prior probability $p$. Then, we generate the feature vector $X$ based on the class label. For $Y = 0$, $X$ follows a $d$-dimensional multivariate normal distribution with a mean vector $\boldsymbol{\mu_0} = (0, \dots, 0)$ and a covariance matrix $S$ defined as $S[i,j] = g^{|i-j|}$. For $Y = 1$, $X$ follows a similar multivariate normal distribution but with a mean vector $\boldsymbol{\mu_1} = \left(1, \frac{1}{2}, \frac{1}{3}, \dots, \frac{1}{d} \right)$ while maintaining the same covariance matrix $S$. This process is repeated to generate $n$ observations.


All of the real datasets were taken from the OpenML \cite{OpenML2013} repository. The datasets were selected based on the criteria of having the amount of features of at least 50\% of the amount of samples, having preferably numerical features, low amount of missing values and a multi-class response variable. The datasets that have met these criteria are as follows:
\begin{itemize}
    \item \textbf{ArrhythmiaDataset} - A binarized version of the Cardiac Arrhythmia Database, where the aim is to determine the type of arrhythmia from the ECG recordings. This dataset contains 279 features, and 452 samples \cite{arrhythmia_5}.
    \item \textbf{SpeechTreatmentDataset} - The dataset containes phonation samples from patients with voice disorders. The aim of the dataset is to assess whether the voice rehabilitation treatment lead to phonations considered 'acceptable' or 'unacceptable'. The dataset contains 309 features, and 126 samples \cite{lsvt_voice_rehabilitation_282}.
    
    \item \textbf{SemeionDataset} - A binarized version of the Semeion Handwritten Digit Dataset, where the aim is to determine the digit from the handwritten samples. This dataset contains 256 features, and 319 samples (only instances 1 and 0 are considered) \cite{semeion_handwritten_digit_178}.
    \item \textbf{DBWorldSubjectsDataset} - The dataset author collected 64 e-mails from DBWorld newsletter and used them to train different algorithms in order to classify between 'announces of conferences' and 'everything else'. The dataset contains 230 features, and 64 samples \cite{dbworld_e-mails_219}.

\end{itemize}

The datasets have been preprocessed by removing colinear features, imputing missing values and standarization (min-max scaling for synthetic datasets and unit-variance scaling for real datasets). The datasets were split into training, validation (for the LogRegCCD algorithm) and test sets. The presented rsults were performed with five predefined seeds, meaning each experiment configuration has been repeated five times.


\subsection{Details about algorithm implementation and applied optimizations}

% TODO: Make it shorter, describe just the optimizations from the paper. Actually double check them with what's in the paper.

Logistic Regression is a machine learning method capable of binary classification. It predicts the probability of an outcome by computing the linear combination of input features and weights, then passing it through the sigmoid function.

The output of the sigmoid function in range $[0,1]$ denotes the probability that given feature vector $x$ belongs to the positive class. What follows the prediction rule is based on the output of the sigmoid function, if it's larger than a set threshold, such as 0.5, we assign the sample to class 1, otherwise assign to class 0.

To fit the model to the training data one needs to minimize the loss function, in this case Binary Cross-Entropy. The weights of the model need to be optimized to find the proper fit, this can be achieved by standard gradient descent algorithm.

It is also important to account for overfitting. Overfitting describes the situation when the trained model can predict samples from the training set very well but struggles on the test set. One of the methods to prevent overfitting of the model to the training data is Lasso Regulaization. In essence during the training process, the model will also minimize the absolute sum of the coefficients in addition to the loss function. This will result in some of the weights being set to zero, effectively reducing the number of features the model is trained on. This can be useful in situations where the number of features is very large and some of them are irrelevant to the prediction task.

Now, to use the Cyclic Coordinate Descent instead of the standard Gradient Descent one needs to minimize the regularized log-likelihood function using a different algorithm for updating model weights. However the authors of the 2010 publication entitled \textit{Regularization Paths for Generalized Linear Models via Coordinate Descent} \cite{Friedman2010} present a more sophisticated approach with certain optimizations.


The logistic regression with lasso regulaization log-likelihood function is approxiamted using a quadratic approximation presented in Formula \ref{eq:quadratic-approximation}. The authors also use a regularization path that starts from largest $\lambda$ where $\beta = 0$ and decreases $\lambda$ gradually, using previous solutions as warm starts. Instead of computing gradients from scratch with each iteration, the authors propose to use covariance updates (Formula \ref{eq:covariance-update}). This in turn allows for a more efficient computation of the gradients. For each feature, the optimization problem simplifies to a minimization problem presented in Formula \ref{eq:feature-minimization}.

\begin{equation}\label{eq:quadratic-approximation}
\ell_Q(\beta_0, \beta) = -\frac{1}{2N} \sum_{i=1}^{N} w_i (z_i - \beta_0 - x_i^T \beta)^2 + C
\end{equation}


\begin{equation}\label{eq:covariance-update}
\sum_{i=1}^{N} x_{ij} r_i = \langle x_j, y \rangle - \sum_{k: \beta_k \neq 0} \langle x_j, x_k \rangle \beta_k
\end{equation}

\begin{equation}\label{eq:feature-minimization}
    \min_{\beta_j} \left[ \frac{1}{2} \sum_{i=1}^{N} w_i \left( z_i - \beta_0 - \sum_{k \neq j} x_{ik} \beta_k - x_{ij} \beta_j \right)^2 + \lambda |\beta_j| \right]
\end{equation}

%TODO: this look sugly, ask if it can stay that way

The algorithm for the cyclic coordinate descent for a given feature $j$ is as follows. Compute partial residuals (excluding $\beta_j$): $r_i = z_i - (\beta_0 + \sum_{k \neq j} x_{ik} \beta_k)$, compute the gradient component $\rho_j$: $\rho_j = \sum_{i=1}^{N} w_i x_{ij} r_i$, apply soft-thresholding for L1 regularization: $\beta_j = \frac{S(\rho_j, \lambda)}{\sum_{i=1}^{N} w_i x_{ij}^2}$, where $S(z, \lambda) = \text{sign}(z) \cdot \max(|z| - \lambda, 0)$ and update $\beta_0$ that is not regularized: $\beta_0 = \frac{\sum_{i=1}^{N} w_i (z_i - x_i^T \beta)}{\sum_{i=1}^{N} w_i}$.


From a high level overview the presented algorithm consists of an outer loop where we decrease $\lambda$ along a regularization path, a middle loop where we update the quadratic approximation using the current $(\beta_0, \beta)$ and an inner loop where we perform coordinate descent on the penalized weighted least squares problem. This process has been translated to Python code available in the attached notebook.



\section{Impact of dataset parameters: n.p,d,g on the performance of LogRegCCD algorithm}

The performance of the LogRegCCD algorithm was evaluated on synthetic datasets with different values of the parameters: class prior probability $p$, number of samples $n$, number of features $d$, and the covariance matrix $g$ parameter defined as $S[i,j] = g^{|i-j|}$. The results are presented in Figure \ref{fig:synthetic-dataset-parameters}. The performance was evaluated using the following metrics: ROC AUC and balanced accuracy. The general conclusion is that the implemented model performs very similarly to the scikit's LogisticRegression algorithm. Furthermore, the effects of the synthetic dataset features on the performance of the models are as follows:

\begin{itemize}
    \item \textbf{p}: As the class prior probability diverges from 0.5 the balanced accuracy decrease. The dataset becomes more imbalanced and the model struggles to predict the minority class. Whereas for ROC AUC the effect of the p parameter does not seem to follow a regular pattern. All models achieve highest ROC AUC for p = 0.5 when the dataset is balanced. For values other than 0.5 ROC AUC drops in non regular way.
    \item \textbf{n}: As the number of samples increases the balanced accuracy increases. The model has more data to learn from and can generalize better. For ROC AUC a similar trend is true up to 1500 samples, after that the ROC AUC stays at a similar level.
    \item \textbf{d}: Both balanced accuracy and ROC AUC increase when the number of features changes from two to five. However, afterwards the performance of the model decreases. This is likely due to the fact that the model is overfitting to the training data. The model has too many features to learn from and it starts to memorize the training data instead of generalizing from it.
    \item \textbf{g}: Here, we see a tilted U shaped curved, as the covarianc increase the balanced accuracy decreases until around 0.7 where it starts to rapidly increase. When the covariance between the features is low they contribute relatively independently to the classification. As the covariance increases the features become more correlated leading to redundant information and the model's performance worsens. However, around $g \approx 0.7$ a transition happens where the features become so correlated that they effectively act as a smaller number of independent features. At this point the model starts performing better again. The ROC AUC metric behaves similarly.
    
\end{itemize}

\begin{figure}[h]
    \centering
  \includegraphics[width=\textwidth]{../results/parameter_facet_grid.png}
    \caption{impact of synthetic dataset parameters on the performance of LogRegCCD algorithm}
    \label{fig:synthetic-dataset-parameters}
  \end{figure}

\section{Benchmark of LogRegCCD with LogisticRegression algorithm}

The overall performance comparison of all three models on the real datasets is available in Figure \ref{fig:real-data-boxplots}. The LogRegCCD algorithm performs nearly identically to the scikit's logistic regression with l1 penalty, although in the case of ArrhythmiaDataset and SpeechTreatmentDataset it achieves higher results in terms of ROC AUC, balanced accuracy and recall while having slightly worse precision meaning that the LogRegCCD is better at predicting the positive class in these specific cases. Interestingly standard logistic regression without penalty performs best in terms of all metrics on the DBWorldSubjectsDataset dataset. However, it is important to keep in mind that this dataset is very small and the model might be overfitting to the training data. Regardless, here the LogRegCCD also achieves satisfactory results with accuracy in the neighbourhood of 80\%. \par

Similar conclusions can be drawn from the overall comparison on synthetic datasets depicted in Figure \ref{fig:comparison-synthetic-dataset}. The LogRegCCD algorithm performs similarly to the scikit's logistic regression with l1 penalty as well as without penalty. Small notable differences are that in terms of accuracy the scikit's model with l1 penalty performs slightly better than the LogRegCCD algorithm, while the LogRegCCD algorithm performs slightly better in terms of precision as well as the scikit's model without penalty achieving slighlt higher balanced accuracy on average. As a result one might conclude that depending on the metric of interest it is worthwhile to consider a different model, however the differences are negligible and it is more important to choose the model that is best suited for the specific dataset and task at hand. \par


\begin{figure}[h]
    \centering
  \includegraphics[width=\textwidth]{../results/real_data_boxplots.png}
    \caption{Boxplots of metrics for real datasets}
    \label{fig:real-data-boxplots}
  \end{figure}

\begin{figure}[h]
    \centering
  \includegraphics[width=\textwidth]{../results/comparison-synthetic-dataset.png}
    \caption{Comparison of LogRegCCD and LogisticRegression on synthetic dataset}
    \label{fig:comparison-synthetic-dataset}
  \end{figure}

% \subsection{Values of coefficients obtained in these two methods}
%TODO: What should be here? Isn't it already contained in the comparison to L1?


\section{Discussion about correctness of the LogRegCCD algorithm}

The previous results already hint at the correctness of the implemented algorithm in terms of it's similar performance to scikit's logistic regression. However, to further investigate the correctness of the algorithm we can look at the log-likelihood function values and coefficient values depending on iteration. The log-likelihood function is a measure of how well the model fits the data. The higher the value, the better the fit. The coefficient values are the weights assigned to each feature in the model. The higher the value, the more important the feature is for the prediction task. \par

At lambda = 0 the log-likelihood function values and coefficient values depending on iteration are presented in Figure \ref{fig:log-likelihood-synthetic-dataset-lambda-0} and Figure \ref{fig:coefficients-synthetic-dataset-lambda-0} respectively. The log-likelihood function values increase with each iteration, meaning that the model is improving with each step. The coefficient values also change with each iteration, but they seem to stabilize after around 100 iterations. Although the itration limit is set to 1000 the process stops at around 250 iterations meaning that the model successfuly converged. This is a good indicator that the implemented model is correct.\par


\begin{figure}[h]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../results/log_likelihood_synthetic_dataset_lambda_0.png}
        \caption{Log likelihood function values depending on iteration for synthetic dataset with $\lambda=0$}
        \label{fig:log-likelihood-synthetic-dataset-lambda-0}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../results/coefficients_synthetic_dataset_lambda_0.png}
        \caption{Coefficient values depending on iteration for synthetic dataset with $\lambda=0$}
        \label{fig:coefficients-synthetic-dataset-lambda-0}
    \end{minipage}
\end{figure}

It is worth to examine the likelihood function values depending on the iteration per every lambda. This is depicted in Figure \ref{fig:log-likelihood-synthetic-dataset}. The log-likelihood function values increase with each iteration for each lambda. However the strongest changes happen for the largest initial lambda value (given the regularization parameter is not too strong). As the lambda decreases the log-likelihood function values increase more slowly. This can be explained by the fact that larger lambda cause more aggressive weights changes translating to large jumps in log-likelihood. As the lambda values get smaller the weights are less constrained and the log-likelihood function values increase more slowly. The same pattern can be observed regarding the values of coefficients (Figure can be found in the attached Python notebook) \par


\begin{figure}
    \centering
  \includegraphics[width=\textwidth]{../results/log_likelihood_synthetic_dataset.png}
    \caption{Log likelihood function values depending on iteration for synthetic dataset}
    \label{fig:log-likelihood-synthetic-dataset}
\end{figure}


Lastly, to investigate the effects of regularization itself the values of coefficients based on lambda have been compared with the values of coefficients obtained from the scikit's logistic regression with l1 penalty. The results are presented in Figure \ref{fig:comparison-synthetic-dataset-redundant-features}. The two plots are almost identical with slight difference at what lambda value the coefficients change from zero. This is likely to the fact that the scikit's logistic regression uses a different algorithm to compute the coefficients. However the differnces are very small and it further strengthens the argument that the implemented algorithm is correct. \par


\begin{figure}[h]
    \centering
  \includegraphics[width=\textwidth]{../results/logistic_regression_l1_logregccd_coefficients_redundant_features.png}
    \caption{Comparison of LogRegCCD and LogisticRegression on synthetic dataset with redundant features}
    \label{fig:comparison-synthetic-dataset-redundant-features}
  \end{figure}


To conclude it is fair to say in the given context that the implemented algorithm is correct, meaning that it is capable of performing logistic regression with lasso regularization and performs similarly to the scikit's implementation. The algorithm is capable of converging to a solution and the results are consistent with the expected behavior of the model. Further information such as the behaviour of the differences in coefficients between the models, or the exact coefficient values in different scenarios is available in the attached Python notebook.


\clearpage 
\listoffigures
% \listoftables
\printbibliography

\end{document}
